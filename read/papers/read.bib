@ARTICLE{10.3389/fncom.2015.00099,
AUTHOR={Diehl, Peter and Cook, Matthew},
TITLE={Unsupervised learning of digit recognition using spike-timing-dependent plasticity},
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={9},
PAGES={99},
YEAR={2015},
URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00099},
DOI={10.3389/fncom.2015.00099},
ISSN={1662-5188},
ABSTRACT={In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most of such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e. conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.}
}

@Article{pmid24653679,
   Author="Tal, A.  and Peled, N.  and Siegelmann, H. T. ",
   Title="{{B}iologically inspired load balancing mechanism in neocortical competitive learning}",
   Journal="Front Neural Circuits",
   Year="2014",
   Volume="8",
   Pages="18"
}

@inproceedings{Maass:2002:MRC:2968618.2968647,
 author = {Maass, Wolfgang and Natschl\"{a}ger, Thomas and Markram, Henry},
 title = {A Model for Real-time Computation in Generic Neural Microcircuits},
 booktitle = {Proceedings of the 15th International Conference on Neural Information Processing Systems},
 series = {NIPS'02},
 year = {2002},
 pages = {229--236},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2968618.2968647},
 acmid = {2968647},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@article{Delahunt2018BiologicalMF,
  title={Biological Mechanisms for Learning: A Computational Model of Olfactory Learning in the Manduca sexta Moth, with Applications to Neural Nets},
  author={Charles B. Delahunt and Jeffrey A. Riffell and J. Nathan Kutz},
  journal={CoRR},
  year={2018},
  volume={abs/1802.02678}
}

@article {Lansdell253351,
	author = {Lansdell, Benjamin James and Kording, Konrad Paul},
	title = {Spiking allows neurons to estimate their causal effect},
	year = {2018},
	doi = {10.1101/253351},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Neural learning aims at the maximization of reward and the typical gradient descent learning is an approximate causal estimator. However real neurons spike based on presynaptic drive, creating discontinuities. The regression discontinuity method, popularized by economics, uses such discontinuities to estimate causal effects. Here we show how the spiking discontinuity can thus reveal the influence of a neuron{\textquoteright}s activity on reward, producing a deep link between simple learning rules and quasi-experimental causal inference.},
	URL = {https://www.biorxiv.org/content/early/2018/01/25/253351},
	eprint = {https://www.biorxiv.org/content/early/2018/01/25/253351.full.pdf},
	journal = {bioRxiv}
}

@article{DBLP:journals/corr/abs-1802-05405,
  author    = {Charles B. Delahunt and
               J. Nathan Kutz},
  title     = {Putting a bug in {ML:} The moth olfactory network learns to read {MNIST}},
  journal   = {CoRR},
  volume    = {abs/1802.05405},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05405},
  archivePrefix = {arXiv},
  eprint    = {1802.05405},
  timestamp = {Thu, 01 Mar 2018 19:20:48 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05405},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
